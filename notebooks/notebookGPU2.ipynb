{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de6d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Updated notebook with CUDA support\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Determine device: use CUDA if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Step 1: Generate synthetic data based on a linear model\n",
    "# y_i ~ N(X_i β, σ²)\n",
    "n = 1000000  # sample size\n",
    "p = 5      # number of predictors (excluding intercept); change as needed\n",
    "\n",
    "if p == 1:\n",
    "    # For a single predictor, use linspace for clarity\n",
    "    X = torch.linspace(0, 10, n).reshape(-1, p)  # Design matrix (without intercept)\n",
    "else:\n",
    "    # For multiple predictors, generate random features in [0, 10)\n",
    "    X = torch.rand(n, p) * 10\n",
    "\n",
    "# True parameters: Choose random intercept and slopes (shape: (p+1, 1))\n",
    "true_beta = torch.randn(p+1, 1)\n",
    "true_sigma = 1.5  # standard deviation of the error term\n",
    "\n",
    "# Generate response variable with noise (μ = Xβ, σ = true_sigma)\n",
    "X_with_intercept = torch.cat([torch.ones(n, 1), X], dim=1)  # Add intercept column\n",
    "mu = X_with_intercept @ true_beta  # μ = Xβ\n",
    "epsilon = torch.randn(n, 1) * true_sigma  # ε ~ N(0, σ²)\n",
    "y = mu + epsilon  # y = Xβ + ε\n",
    "\n",
    "# Move data (and true parameters if needed for plotting) to the selected device\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "true_beta = true_beta.to(device)\n",
    "\n",
    "# Step 2: Define the linear model as y ~ N(Xβ, σ²)\n",
    "# The model deduces the input dimension from an example design matrix.\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, example_x):\n",
    "        super(LinearModel, self).__init__()\n",
    "        input_dim = example_x.shape[-1]  # Deduce the number of predictors\n",
    "        self.beta = nn.Parameter(torch.randn(input_dim + 1, 1))  # β vector includes intercept\n",
    "        self.log_sigma = nn.Parameter(torch.tensor(0.0))  # log(σ) for numerical stability\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_with_intercept = torch.cat([torch.ones(x.size(0), 1, device=x.device), x], dim=1)\n",
    "        mu = x_with_intercept @ self.beta  # μ = Xβ\n",
    "        return mu\n",
    "    \n",
    "    def sigma(self):\n",
    "        return torch.exp(self.log_sigma)  # Convert log(σ) to σ\n",
    "    \n",
    "    def negative_log_likelihood(self, x, y):\n",
    "        mu = self(x)\n",
    "        sigma = self.sigma()\n",
    "        # Negative log-likelihood for normal distribution\n",
    "        return (torch.log(sigma) + 0.5 * ((y - mu) / sigma)**2).sum() + 0.5 * np.log(2 * np.pi) * y.numel()\n",
    "\n",
    "# Step 3: Instantiate model and optimizer using X to deduce input dimension\n",
    "model = LinearModel(X)\n",
    "model = model.to(device)  # Move model to the selected device\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Step 4: Train the model with timing\n",
    "num_epochs = 5000\n",
    "losses = []\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute negative log-likelihood\n",
    "    loss = model.negative_log_likelihood(X, y)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], NLL: {loss.item():.4f}')\n",
    "\n",
    "elapsed_time = time.time() - start_time  # Stop the timer\n",
    "print(f\"Training completed in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Step 5: Extract learned parameters\n",
    "learned_beta = model.beta.data.flatten()\n",
    "learned_sigma = model.sigma().item()\n",
    "\n",
    "print(f'Learned parameters:')\n",
    "print(f'β = {learned_beta.tolist()}')\n",
    "print(f'σ = {learned_sigma:.4f}')\n",
    "print(f'True parameters:')\n",
    "print(f'β = {true_beta.flatten().tolist()}')\n",
    "print(f'σ = {true_sigma:.4f}')\n",
    "\n",
    "# Step 6: Plot results (only if p==1)\n",
    "# Note: Plotting is done on the CPU so ensure data is moved to CPU when plotting\n",
    "if p == 1:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Convert tensors back to CPU for plotting\n",
    "    X_cpu = X.cpu()\n",
    "    y_cpu = y.cpu()\n",
    "    true_beta_cpu = true_beta.cpu()\n",
    "    \n",
    "    # Plot training data\n",
    "    plt.scatter(X_cpu.numpy(), y_cpu.numpy(), label='Observations', s=1)\n",
    "    \n",
    "    # Plot the true regression line\n",
    "    x_range = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "    x_range_with_intercept = np.column_stack([np.ones(100), x_range])\n",
    "    plt.plot(x_range, x_range_with_intercept @ true_beta_cpu.numpy(), 'r', \n",
    "             label=f'True: y = {true_beta_cpu[0].item():.2f} + {true_beta_cpu[1].item():.2f}x')\n",
    "    \n",
    "    # Plot the fitted regression line\n",
    "    with torch.no_grad():\n",
    "        # Move x_range to device before model prediction, then back to cpu for plotting\n",
    "        x_range_tensor = torch.FloatTensor(x_range).to(device)\n",
    "        y_pred = model(x_range_tensor).cpu()\n",
    "    plt.plot(x_range, y_pred.numpy(), 'g--', \n",
    "             label=f'Fitted: y = {learned_beta[0].item():.2f} + {learned_beta[1].item():.2f}x')\n",
    "    \n",
    "    # Plot confidence bands (μ ± 2σ)\n",
    "    # For confidence bands, compute predictions using learned_beta on CPU\n",
    "    learned_beta_cpu = learned_beta.cpu() if learned_beta.device.type != \"cpu\" else learned_beta\n",
    "    y_pred_band = x_range_with_intercept @ learned_beta_cpu.unsqueeze(1).numpy()\n",
    "    plt.fill_between(\n",
    "        x_range.flatten(),\n",
    "        (y_pred_band - 2 * learned_sigma).reshape(-1),\n",
    "        (y_pred_band + 2 * learned_sigma).reshape(-1),\n",
    "        alpha=0.2, color='g', label=f'95% Confidence Band (σ = {learned_sigma:.2f})'\n",
    "    )\n",
    "    \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.title('Gaussian Regression with Fixed Effects (β) and Standard Deviation (σ)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping plot since p > 1 (plotting only works for a single predictor).\")\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(num_epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Negative Log-Likelihood')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2662fa8",
   "metadata": {},
   "source": [
    "$$y \\sim \\mathcal{N}(\\mu, \\sigma^2) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d94117b",
   "metadata": {},
   "source": [
    "$$ p(y \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi},\\sigma}\\exp\\left(-\\frac{1}{2}\\left(\\frac{y-\\mu}{\\sigma}\\right)^2\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514cd268",
   "metadata": {},
   "source": [
    "$$ -\\log p(y \\mid \\mu, \\sigma) = \\frac{1}{2}\\log(2\\pi) + \\log(\\sigma) + \\frac{1}{2}\\left(\\frac{y-\\mu}{\\sigma}\\right)^2 $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
